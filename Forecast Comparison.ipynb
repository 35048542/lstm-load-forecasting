{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import pytz\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "\n",
    "from lstm_load_forecasting import data, lstm\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TBATS Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tbats_fc = pd.read_csv(os.path.join('Data', 'tbats_forecast_01022017-h5000.csv'))\n",
    "\n",
    "starting = datetime.datetime(2017,2,1,0,0,0,0, tzinfo=pytz.utc )\n",
    "forecasts = pd.DataFrame(data={\"tbats_forecast\": tbats_fc['tbats_fc'].values}, index=pd.date_range(starting, periods=5000, freq='60min'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Load and ENTSOE Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv(os.path.join('Data', 'fulldataset.csv'), sep=';', usecols=[0,1,2], parse_dates=[0], index_col = 0)\n",
    "path = os.path.join('Data', 'fulldataset.csv')\n",
    "entsoe = data.load_dataset(path=path, modules=['entsoe'])\n",
    "actual = data.load_dataset(path=path, modules=['actual'])\n",
    "forecasts = forecasts.join(entsoe)\n",
    "forecasts = forecasts.join(actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best models based on test results. For comparison\n",
    "res_path = os.path.abspath('results/')\n",
    "model_dir = os.path.abspath('models/')\n",
    "date = '20170508'\n",
    "starting = datetime.datetime(2017,2,1,0,0,0,0, tzinfo=pytz.utc )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model 6 (All available data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df6 = data.load_dataset(path=path, modules=['all'])\n",
    "df6_scaled = df6.copy()\n",
    "df6_scaled = df6_scaled.dropna()\n",
    "\n",
    "# Get all float type columns\n",
    "floats = [key for key in dict(df6_scaled.dtypes) if dict(df6_scaled.dtypes)[key] in ['float64']]\n",
    "scaler = StandardScaler()\n",
    "scaled_columns = scaler.fit_transform(df6_scaled[floats])\n",
    "df6_scaled[floats] = scaled_columns\n",
    "\n",
    "df6_train = df6_scaled.loc[(df6_scaled.index < starting)].copy()\n",
    "#df6_train = df6_scaled.loc[(df6_scaled.index < starting) & (df6_scaled.index > starting - pd.DateOffset(months=15))].copy()\n",
    "df6_test = df6_scaled.loc[df6_scaled.index >= starting].copy()\n",
    "y_train = df6_train['actual'].copy()\n",
    "X_train = df6_train.drop('actual', 1).copy()\n",
    "y_test = df6_test['actual'].copy()\n",
    "X_test = df6_test.drop('actual', 1).copy()\n",
    "\n",
    "valid_results_6 = pd.read_csv(os.path.join(res_path, 'notebook_06/', str('model6_results_' + date + '.csv')), delimiter=';')\n",
    "test_results_6 = pd.read_csv(os.path.join(res_path, 'notebook_06/', str('model6_test_results' + date + '.csv')), delimiter=';')\n",
    "test_results_6 = test_results_6.sort_values('Mean absolute error', ascending=True)\n",
    "best_model_6 = test_results_6.loc[0]['Model name']\n",
    "\n",
    "config = valid_results_6.loc[valid_results_6['model_name'] == best_model_6]\n",
    "batch_size = int(config['batch_train'].values[0])\n",
    "size = int(y_test.shape[0] / batch_size)\n",
    "\n",
    "layers = literal_eval(config['config'].values[0])\n",
    "layers = layers['layers']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 1 of 18241 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "Train on 14592 samples, validate on 3648 samples\n",
      "Epoch 1/25\n",
      "14592/14592 [==============================] - 12s - loss: 0.7693 - mean_absolute_error: 0.6265 - val_loss: 0.5191 - val_mean_absolute_error: 0.5773\n",
      "Epoch 2/25\n",
      "14592/14592 [==============================] - 10s - loss: 0.5575 - mean_absolute_error: 0.4920 - val_loss: 0.2337 - val_mean_absolute_error: 0.3777\n",
      "Epoch 3/25\n",
      "14592/14592 [==============================] - 11s - loss: 0.4997 - mean_absolute_error: 0.4520 - val_loss: 0.2698 - val_mean_absolute_error: 0.4062\n",
      "Epoch 4/25\n",
      "14592/14592 [==============================] - 11s - loss: 0.4647 - mean_absolute_error: 0.4306 - val_loss: 0.2210 - val_mean_absolute_error: 0.3646\n",
      "Epoch 5/25\n",
      "14592/14592 [==============================] - 12s - loss: 0.4529 - mean_absolute_error: 0.4195 - val_loss: 0.2026 - val_mean_absolute_error: 0.3481\n",
      "Epoch 6/25\n",
      "14592/14592 [==============================] - 13s - loss: 0.4375 - mean_absolute_error: 0.4096 - val_loss: 0.1806 - val_mean_absolute_error: 0.3245\n",
      "Epoch 7/25\n",
      "14592/14592 [==============================] - 12s - loss: 0.4284 - mean_absolute_error: 0.4025 - val_loss: 0.1533 - val_mean_absolute_error: 0.2990\n",
      "Epoch 8/25\n",
      "14592/14592 [==============================] - 13s - loss: 0.4162 - mean_absolute_error: 0.3938 - val_loss: 0.1335 - val_mean_absolute_error: 0.2800\n",
      "Epoch 9/25\n",
      "14592/14592 [==============================] - 13s - loss: 0.4071 - mean_absolute_error: 0.3861 - val_loss: 0.1217 - val_mean_absolute_error: 0.2702\n",
      "Epoch 10/25\n",
      "14592/14592 [==============================] - 13s - loss: 0.4000 - mean_absolute_error: 0.3805 - val_loss: 0.1182 - val_mean_absolute_error: 0.2679\n",
      "Epoch 11/25\n",
      "14592/14592 [==============================] - 14s - loss: 0.3905 - mean_absolute_error: 0.3731 - val_loss: 0.1307 - val_mean_absolute_error: 0.2818\n",
      "Epoch 12/25\n",
      "14592/14592 [==============================] - 13s - loss: 0.3831 - mean_absolute_error: 0.3675 - val_loss: 0.1381 - val_mean_absolute_error: 0.2905\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "model6 = lstm.create_model(layers=layers, sample_size=X_train.shape[0], batch_size=config['batch_train'].values, timesteps=1, features=X_train.shape[1], loss='mse', optimizer='adam')\n",
    "history = lstm.train_model(model=model6, mode='fit', y=y_train, X=X_train, \n",
    "                           batch_size=batch_size, timesteps=1, epochs=25, \n",
    "                           rearrange=False, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2368/2432 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "scaled_predictions = lstm.get_predictions(model=model6, X=X_test[0:size*batch_size], batch_size=batch_size, timesteps=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = scaler.mean_[0]\n",
    "sigma = scaler.scale_[0]\n",
    "\n",
    "mod6_predictions = mu + sigma*scaled_predictions\n",
    "df_mod6 = pd.DataFrame(data={\"model6\": mod6_predictions.flatten()}, index=pd.date_range(starting, periods=mod6_predictions.shape[0], freq='60min'))\n",
    "if 'model6' in forecasts.columns:\n",
    "    forecasts = forecasts.drop('model6', 1)\n",
    "forecasts = forecasts.join(df_mod6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model 3 (Calendar only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = data.load_dataset(path=path, modules=['actual', 'calendar'])\n",
    "df3_scaled = df3.copy()\n",
    "df3_scaled = df3_scaled.dropna()\n",
    "\n",
    "# Get all float type columns\n",
    "floats = [key for key in dict(df3_scaled.dtypes) if dict(df3_scaled.dtypes)[key] in ['float64']]\n",
    "scaler = StandardScaler()\n",
    "scaled_columns = scaler.fit_transform(df3_scaled[floats])\n",
    "df3_scaled[floats] = scaled_columns\n",
    "\n",
    "df3_train = df3_scaled.loc[(df3_scaled.index < starting)].copy()\n",
    "#df3_train = df3_scaled.loc[(df3_scaled.index < starting) & (df3_scaled.index > starting - pd.DateOffset(months=15))].copy()\n",
    "df3_test = df3_scaled.loc[df3_scaled.index >= starting].copy()\n",
    "y_train = df3_train['actual'].copy()\n",
    "X_train = df3_train.drop('actual', 1).copy()\n",
    "y_test = df3_test['actual'].copy()\n",
    "X_test = df3_test.drop('actual', 1).copy()\n",
    "\n",
    "valid_results_3 = pd.read_csv(os.path.join(res_path, 'notebook_03/', str('model3_results_' + date + '.csv')), delimiter=';')\n",
    "test_results_3 = pd.read_csv(os.path.join(res_path, 'notebook_03/', str('model3_test_results' + date + '.csv')), delimiter=';')\n",
    "test_results_3 = test_results_3.sort_values('Mean absolute error', ascending=True)\n",
    "best_model_3 = test_results_3.loc[0]['Model name']\n",
    "\n",
    "config = valid_results_3.loc[valid_results_3['model_name'] == best_model_3]\n",
    "batch_size = int(config['batch_train'].values[0])\n",
    "size = int(y_test.shape[0] / batch_size)\n",
    "\n",
    "layers = literal_eval(config['config'].values[0])\n",
    "layers = layers['layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 1 of 18241 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "Train on 14592 samples, validate on 3648 samples\n",
      "Epoch 1/25\n",
      "14592/14592 [==============================] - 2s - loss: 1.0104 - mean_absolute_error: 0.7481 - val_loss: 0.5073 - val_mean_absolute_error: 0.5655\n",
      "Epoch 2/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.6942 - mean_absolute_error: 0.5777 - val_loss: 0.2629 - val_mean_absolute_error: 0.4106\n",
      "Epoch 3/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.5460 - mean_absolute_error: 0.4976 - val_loss: 0.2194 - val_mean_absolute_error: 0.3757\n",
      "Epoch 4/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.5161 - mean_absolute_error: 0.4791 - val_loss: 0.2033 - val_mean_absolute_error: 0.3613\n",
      "Epoch 5/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.5040 - mean_absolute_error: 0.4684 - val_loss: 0.1905 - val_mean_absolute_error: 0.3493\n",
      "Epoch 6/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.4945 - mean_absolute_error: 0.4600 - val_loss: 0.1799 - val_mean_absolute_error: 0.3386\n",
      "Epoch 7/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.4858 - mean_absolute_error: 0.4531 - val_loss: 0.1724 - val_mean_absolute_error: 0.3307\n",
      "Epoch 8/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.4781 - mean_absolute_error: 0.4472 - val_loss: 0.1671 - val_mean_absolute_error: 0.3249\n",
      "Epoch 9/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.4711 - mean_absolute_error: 0.4420 - val_loss: 0.1634 - val_mean_absolute_error: 0.3210\n",
      "Epoch 10/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.4649 - mean_absolute_error: 0.4375 - val_loss: 0.1611 - val_mean_absolute_error: 0.3183\n",
      "Epoch 11/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.4596 - mean_absolute_error: 0.4337 - val_loss: 0.1601 - val_mean_absolute_error: 0.3172\n",
      "Epoch 12/25\n",
      "14592/14592 [==============================] - 2s - loss: 0.4552 - mean_absolute_error: 0.4304 - val_loss: 0.1600 - val_mean_absolute_error: 0.3177\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "model3 = lstm.create_model(layers=layers, sample_size=X_train.shape[0], batch_size=config['batch_train'].values, timesteps=1, features=X_train.shape[1], loss='mse', optimizer='adam')\n",
    "history = lstm.train_model(model=model3, mode='fit', y=y_train, X=X_train, \n",
    "                           batch_size=batch_size, timesteps=1, epochs=25, \n",
    "                           rearrange=False, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/2432 [==================>...........] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "scaled_predictions = lstm.get_predictions(model=model3, X=X_test[0:size*batch_size], batch_size=batch_size, timesteps=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = scaler.mean_[0]\n",
    "sigma = scaler.scale_[0]\n",
    "\n",
    "mod3_predictions = mu + sigma*scaled_predictions\n",
    "df_mod3 = pd.DataFrame(data={\"model3\": mod3_predictions.flatten()}, index=pd.date_range(starting, periods=mod3_predictions.shape[0], freq='60min'))\n",
    "if 'model3' in forecasts.columns:\n",
    "    forecasts = forecasts.drop('model3', 1)\n",
    "forecasts = forecasts.join(df_mod3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table with Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "      &     tbats &   entsoe &   m6-all &   m3-calendar \\\\\n",
      "\\midrule\n",
      " MSE  & 1014545.8 & 439829.7 & 195661.9 &      244965.0 \\\\\n",
      " MAE  &     852.4 &    530.6 &    357.7 &         402.7 \\\\\n",
      " MAPE &      13.2 &      7.9 &      5.3 &           6.1 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "forecasts = forecasts.dropna()\n",
    "\n",
    "results = {}\n",
    "results[''] = ['MSE', 'MAE', 'MAPE']\n",
    "results['tbats'] = [mean_squared_error(forecasts['actual'], forecasts['tbats_forecast']), \n",
    "                    mean_absolute_error(forecasts['actual'], forecasts['tbats_forecast']),\n",
    "                    mean_absolute_percentage_error(forecasts['actual'], forecasts['tbats_forecast'])\n",
    "                   ]\n",
    "results['entsoe'] = [mean_squared_error(forecasts['actual'], forecasts['entsoe']), \n",
    "                     mean_absolute_error(forecasts['actual'], forecasts['entsoe']),\n",
    "                     mean_absolute_percentage_error(forecasts['actual'], forecasts['entsoe'])\n",
    "                    ]\n",
    "results['m6-all'] = [mean_squared_error(forecasts['actual'], forecasts['model6']), \n",
    "                     mean_absolute_error(forecasts['actual'], forecasts['model6']),\n",
    "                     mean_absolute_percentage_error(forecasts['actual'], forecasts['model6'])\n",
    "                    ]\n",
    "results['m3-calendar'] = [mean_squared_error(forecasts['actual'], forecasts['model3']), \n",
    "                          mean_absolute_error(forecasts['actual'], forecasts['model3']),\n",
    "                          mean_absolute_percentage_error(forecasts['actual'], forecasts['model3'])\n",
    "                         ]\n",
    "\n",
    "print(tabulate(results, headers='keys', numalign=\"right\", tablefmt='latex_booktabs', floatfmt=\".1f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-60a46ace8a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecasts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecasts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'entsoe'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ENTSOE Forecast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecasts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecasts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'actual'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Actual Load'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecasts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecasts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tbats_forecast'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'TBATS Forecast'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforecasts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforecasts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model6'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Model 6 (All)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(forecasts.index, forecasts['entsoe'], label='ENTSOE Forecast')\n",
    "plt.plot(forecasts.index, forecasts['actual'], label='Actual Load')\n",
    "plt.plot(forecasts.index, forecasts['tbats_forecast'], label='TBATS Forecast')\n",
    "plt.plot(forecasts.index, forecasts['model6'], label='Model 6 (All)')\n",
    "plt.plot(forecasts.index, forecasts['model3'], label='Model 3 (Calendar)')\n",
    "plt.title('Forecast Comparison: Test Data')\n",
    "plt.ylabel('Electricity load (in MW)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
