{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Rolling Forecast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "\n",
    "import pytz\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tabulate import tabulate\n",
    "\n",
    "from lstm_load_forecasting import data, lstm\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from dateutil.tz import tzutc\n",
    "\n",
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timezone = pytz.timezone('Europe/Zurich')\n",
    "starting = timezone.localize(datetime.datetime(2017,2,1,0,0,0,0))\n",
    "path = os.path.join('Data', 'fulldataset.csv')\n",
    "res_path = os.path.abspath('results/')\n",
    "model_dir = os.path.abspath('models/')\n",
    "date = '20170508'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = data.load_dataset(path=path, modules=['actual', 'calendar'])\n",
    "df3_scaled = df3.copy()\n",
    "df3_scaled = df3_scaled.dropna()\n",
    "\n",
    "# Get all float type columns\n",
    "floats = [key for key in dict(df3_scaled.dtypes) if dict(df3_scaled.dtypes)[key] in ['float64']]\n",
    "scaler = StandardScaler()\n",
    "scaled_columns = scaler.fit_transform(df3_scaled[floats])\n",
    "df3_scaled[floats] = scaled_columns\n",
    "\n",
    "df3_train = df3_scaled.loc[(df3_scaled.index < starting )].copy()\n",
    "#df3_train = df3_scaled.loc[(df3_scaled.index < starting) & (df3_scaled.index > starting - pd.DateOffset(months=15))].copy()\n",
    "df3_test = df3_scaled.loc[df3_scaled.index >= starting].copy()\n",
    "y_train = df3_train['actual'].copy()\n",
    "X_train = df3_train.drop('actual', 1).copy()\n",
    "y_test = df3_test['actual'].copy()\n",
    "X_test = df3_test.drop('actual', 1).copy()\n",
    "\n",
    "valid_results_3 = pd.read_csv(os.path.join(res_path, 'notebook_03/', str('model3_results_' + date + '.csv')), delimiter=';')\n",
    "test_results_3 = pd.read_csv(os.path.join(res_path, 'notebook_03/', str('model3_test_results' + date + '.csv')), delimiter=';')\n",
    "test_results_3 = test_results_3.sort_values('Mean absolute error', ascending=True)\n",
    "best_model_3 = test_results_3.loc[0]['Model name']\n",
    "\n",
    "config = valid_results_3.loc[valid_results_3['model_name'] == best_model_3]\n",
    "batch_size = int(config['batch_train'].values[0])\n",
    "size = int(y_test.shape[0] / batch_size)\n",
    "\n",
    "layers = literal_eval(config['config'].values[0])\n",
    "layers = layers['layers']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First training of network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14592 samples, validate on 3648 samples\n",
      "Epoch 1/25\n",
      "14592/14592 [==============================] - 9s - loss: 0.6594 - mean_absolute_error: 0.5568 - val_loss: 0.3774 - val_mean_absolute_error: 0.4885\n",
      "Epoch 2/25\n",
      "14592/14592 [==============================] - 8s - loss: 0.4699 - mean_absolute_error: 0.4398 - val_loss: 0.3216 - val_mean_absolute_error: 0.4752\n",
      "Epoch 3/25\n",
      "14592/14592 [==============================] - 8s - loss: 0.4441 - mean_absolute_error: 0.4164 - val_loss: 0.3105 - val_mean_absolute_error: 0.4631\n",
      "Epoch 4/25\n",
      "14592/14592 [==============================] - 8s - loss: 0.4322 - mean_absolute_error: 0.4056 - val_loss: 0.2896 - val_mean_absolute_error: 0.4432\n",
      "Epoch 5/25\n",
      "14592/14592 [==============================] - 8s - loss: 0.4243 - mean_absolute_error: 0.3989 - val_loss: 0.2748 - val_mean_absolute_error: 0.4295\n",
      "Epoch 6/25\n",
      "14592/14592 [==============================] - 8s - loss: 0.4186 - mean_absolute_error: 0.3946 - val_loss: 0.2687 - val_mean_absolute_error: 0.4247\n",
      "Epoch 7/25\n",
      "14592/14592 [==============================] - 8s - loss: 0.4140 - mean_absolute_error: 0.3914 - val_loss: 0.2712 - val_mean_absolute_error: 0.4282\n",
      "Epoch 8/25\n",
      "14592/14592 [==============================] - 8s - loss: 0.4103 - mean_absolute_error: 0.3891 - val_loss: 0.2754 - val_mean_absolute_error: 0.4337\n",
      "Epoch 9/25\n",
      "14592/14592 [==============================] - 9s - loss: 0.4077 - mean_absolute_error: 0.3876 - val_loss: 0.2747 - val_mean_absolute_error: 0.4348\n",
      "Epoch 00008: early stopping\n"
     ]
    }
   ],
   "source": [
    "#batch_size = config['batch_train'].values \n",
    "batch_size = 8\n",
    "model3 = lstm.create_model(layers=layers, sample_size=X_train.shape[0], batch_size=batch_size, timesteps=1, \n",
    "                           features=X_train.shape[1], loss='mse', optimizer='adam')\n",
    "history = lstm.train_model(model=model3, mode='fit', y=y_train, X=X_train, \n",
    "                           batch_size=batch_size, timesteps=1, epochs=25, \n",
    "                           rearrange=False, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through windows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/48 [====>.........................] - ETA: 0sWarnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 7 of 47 number of obs.\n",
      " 8/40 [=====>........................] - ETA: 0sWarnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 7 of 47 number of obs.\n",
      " 8/48 [====>.........................] - ETA: 0sWarnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 7 of 39 number of obs.\n",
      " 8/32 [======>.......................] - ETA: 0sWarnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 7 of 15 number of obs.\n",
      "8/8 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "starting_loc = starting\n",
    "starting_utc = starting.astimezone(pytz.utc)\n",
    "ending_utc = df3_test.tail(1).index[0]\n",
    "ending_loc = ending_utc.tz_convert('Europe/Zurich')\n",
    "day_changes = pd.date_range(start=starting_loc, end=ending_loc, freq='24h', tz=\"Europe/Zurich\")\n",
    "\n",
    "predictions = pd.DataFrame(index=pd.date_range(start=starting_loc, end=ending_loc, normalize=True, freq='60min', tz='Europe/Zurich'))\n",
    "predictions.index = predictions.index.tz_convert('utc')\n",
    "\n",
    "for idx, hour in enumerate(day_changes):\n",
    "    if idx == 0:\n",
    "        pass\n",
    "    else:\n",
    "        dh = df3_test.loc[(df3_test.index >= hour - pd.DateOffset(days=1)) & (df3_test.index < hour)]\n",
    "        y_train = dh['actual']\n",
    "        X_train = dh.drop('actual', 1)\n",
    "        history = lstm.train_model(model=model3, mode='fit', y=y_train, X=X_train, \n",
    "                                   batch_size=batch_size, timesteps=1, epochs=13, rearrange=False, validation_split=0,\n",
    "                                   verbose=0, early_stopping=False)\n",
    "    \n",
    "    df = df3_test.loc[(df3_test.index >= hour) & (df3_test.index < hour + pd.DateOffset(days=2))]\n",
    "    df = df.drop('actual', 1)\n",
    "    X_predict = df\n",
    "    scaled_predictions = lstm.get_predictions(model=model3, X=X_predict, batch_size=batch_size, timesteps=1, verbose=1)\n",
    "    hour_utc = hour.tz_convert('utc')\n",
    "    window = pd.date_range(start=hour_utc, periods=len(scaled_predictions), freq='60min', tz='UTC')\n",
    "    result = pd.DataFrame(data={\"model3\": scaled_predictions.flatten()}, index=window)\n",
    "    predictions = predictions.combine_first(result)\n",
    "\n",
    "mu = scaler.mean_[0]\n",
    "sigma = scaler.scale_[0]\n",
    "model3_predictions = mu + sigma*predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df6 = data.load_dataset(path=path, modules=['all'])\n",
    "df6_scaled = df6.copy()\n",
    "df6_scaled = df6_scaled.dropna()\n",
    "\n",
    "# Get all float type columns\n",
    "floats = [key for key in dict(df6_scaled.dtypes) if dict(df6_scaled.dtypes)[key] in ['float64']]\n",
    "scaler = StandardScaler()\n",
    "scaled_columns = scaler.fit_transform(df6_scaled[floats])\n",
    "df6_scaled[floats] = scaled_columns\n",
    "\n",
    "df6_train = df6_scaled.loc[(df6_scaled.index < starting )].copy()\n",
    "#df6_train = df6_scaled.loc[(df6_scaled.index < starting) & (df6_scaled.index > starting - pd.DateOffset(months=15))].copy()\n",
    "df6_test = df6_scaled.loc[df6_scaled.index >= starting].copy()\n",
    "y_train = df6_train['actual'].copy()\n",
    "X_train = df6_train.drop('actual', 1).copy()\n",
    "y_test = df6_test['actual'].copy()\n",
    "X_test = df6_test.drop('actual', 1).copy()\n",
    "\n",
    "valid_results_6 = pd.read_csv(os.path.join(res_path, 'notebook_06/', str('model6_results_' + date + '.csv')), delimiter=';')\n",
    "test_results_6 = pd.read_csv(os.path.join(res_path, 'notebook_06/', str('model6_test_results' + date + '.csv')), delimiter=';')\n",
    "test_results_6 = test_results_6.sort_values('Mean absolute error', ascending=True)\n",
    "best_model_6 = test_results_6.loc[0]['Model name']\n",
    "\n",
    "config = valid_results_6.loc[valid_results_6['model_name'] == best_model_6]\n",
    "batch_size = int(config['batch_train'].values[0])\n",
    "size = int(y_test.shape[0] / batch_size)\n",
    "\n",
    "layers = literal_eval(config['config'].values[0])\n",
    "layers = layers['layers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 14592 samples, validate on 3648 samples\n",
      "Epoch 1/25\n",
      "14592/14592 [==============================] - 25s - loss: 0.7480 - mean_absolute_error: 0.6211 - val_loss: 0.4897 - val_mean_absolute_error: 0.5612\n",
      "Epoch 2/25\n",
      "14592/14592 [==============================] - 25s - loss: 0.5300 - mean_absolute_error: 0.4766 - val_loss: 0.2869 - val_mean_absolute_error: 0.4176\n",
      "Epoch 3/25\n",
      "14592/14592 [==============================] - 25s - loss: 0.4454 - mean_absolute_error: 0.4210 - val_loss: 0.2388 - val_mean_absolute_error: 0.3817\n",
      "Epoch 4/25\n",
      "14592/14592 [==============================] - 26s - loss: 0.4207 - mean_absolute_error: 0.3990 - val_loss: 0.2467 - val_mean_absolute_error: 0.3874\n",
      "Epoch 5/25\n",
      "14592/14592 [==============================] - 27s - loss: 0.4095 - mean_absolute_error: 0.3892 - val_loss: 0.2286 - val_mean_absolute_error: 0.3730\n",
      "Epoch 6/25\n",
      "14592/14592 [==============================] - 27s - loss: 0.3942 - mean_absolute_error: 0.3777 - val_loss: 0.1919 - val_mean_absolute_error: 0.3429\n",
      "Epoch 7/25\n",
      "14592/14592 [==============================] - 25s - loss: 0.3895 - mean_absolute_error: 0.3729 - val_loss: 0.1931 - val_mean_absolute_error: 0.3424\n",
      "Epoch 8/25\n",
      "14592/14592 [==============================] - 26s - loss: 0.3779 - mean_absolute_error: 0.3639 - val_loss: 0.1744 - val_mean_absolute_error: 0.3242\n",
      "Epoch 9/25\n",
      "14592/14592 [==============================] - 26s - loss: 0.3751 - mean_absolute_error: 0.3597 - val_loss: 0.1520 - val_mean_absolute_error: 0.3048\n",
      "Epoch 10/25\n",
      "14592/14592 [==============================] - 25s - loss: 0.3652 - mean_absolute_error: 0.3510 - val_loss: 0.1556 - val_mean_absolute_error: 0.3093\n",
      "Epoch 11/25\n",
      "14592/14592 [==============================] - 24s - loss: 0.3610 - mean_absolute_error: 0.3487 - val_loss: 0.1579 - val_mean_absolute_error: 0.3118\n",
      "Epoch 12/25\n",
      "14592/14592 [==============================] - 26s - loss: 0.3552 - mean_absolute_error: 0.3438 - val_loss: 0.1574 - val_mean_absolute_error: 0.3146\n",
      "Epoch 00011: early stopping\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "model6 = lstm.create_model(layers=layers, sample_size=X_train.shape[0], batch_size=batch_size, timesteps=1, \n",
    "                           features=X_train.shape[1], loss='mse', optimizer='adam')\n",
    "history = lstm.train_model(model=model6, mode='fit', y=y_train, X=X_train, \n",
    "                           batch_size=batch_size, timesteps=1, epochs=25, \n",
    "                           rearrange=False, validation_split=0.2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/48 [====>.........................] - ETA: 0sWarnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 7 of 47 number of obs.\n",
      " 8/40 [=====>........................] - ETA: 0sWarnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 7 of 47 number of obs.\n",
      " 8/48 [====>.........................] - ETA: 0sWarnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 7 of 39 number of obs.\n",
      " 8/32 [======>.......................] - ETA: 0sWarnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 7 of 15 number of obs.\n",
      "8/8 [==============================] - 0s\n"
     ]
    }
   ],
   "source": [
    "starting_loc = starting\n",
    "starting_utc = starting.astimezone(pytz.utc)\n",
    "ending_utc = df6_test.tail(1).index[0]\n",
    "ending_loc = ending_utc.tz_convert('Europe/Zurich')\n",
    "day_changes = pd.date_range(start=starting_loc, end=ending_loc, freq='24h', tz=\"Europe/Zurich\")\n",
    "\n",
    "predictions = pd.DataFrame(index=pd.date_range(start=starting_loc, end=ending_loc, normalize=True, freq='60min', tz='Europe/Zurich'))\n",
    "predictions.index = predictions.index.tz_convert('utc')\n",
    "\n",
    "for idx, hour in enumerate(day_changes):\n",
    "    if idx == 0:\n",
    "        pass\n",
    "    else:\n",
    "        dh = df6_test.loc[(df6_test.index >= hour - pd.DateOffset(days=1)) & (df6_test.index < hour)]\n",
    "        y_train = dh['actual']\n",
    "        X_train = dh.drop('actual', 1)\n",
    "        history = lstm.train_model(model=model6, mode='fit', y=y_train, X=X_train, \n",
    "                                   batch_size=batch_size, timesteps=1, epochs=13, rearrange=False, validation_split=0,\n",
    "                                   verbose=0, early_stopping=False)\n",
    "    \n",
    "    df = df6_test.loc[(df6_test.index >= hour) & (df6_test.index < hour + pd.DateOffset(days=2))]\n",
    "    df = df.drop('actual', 1)\n",
    "    X_predict = df\n",
    "    scaled_predictions = lstm.get_predictions(model=model6, X=X_predict, batch_size=batch_size, timesteps=1, verbose=1)\n",
    "    hour_utc = hour.tz_convert('utc')\n",
    "    window = pd.date_range(start=hour_utc, periods=len(scaled_predictions), freq='60min', tz='UTC')\n",
    "    result = pd.DataFrame(data={\"model6\": scaled_predictions.flatten()}, index=window)\n",
    "    predictions = predictions.combine_first(result)\n",
    "\n",
    "mu = scaler.mean_[0]\n",
    "sigma = scaler.scale_[0]\n",
    "model6_predictions = mu + sigma*predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+---------------+\n",
      "|      |     entsoe |     m6-all |   m3-calendar |\n",
      "+======+============+============+===============+\n",
      "| MSE  | 439885.823 | 133013.981 |    146157.854 |\n",
      "+------+------------+------------+---------------+\n",
      "| MAE  |    530.497 |    279.142 |       300.047 |\n",
      "+------+------------+------------+---------------+\n",
      "| MAPE |      7.862 |      4.122 |         4.427 |\n",
      "+------+------------+------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "dfc = data.load_dataset(path=path, modules=['actual', 'entsoe'])\n",
    "forecasts = model3_predictions\n",
    "forecasts = forecasts.join(model6_predictions)\n",
    "forecasts = forecasts.join(dfc)\n",
    "forecasts = forecasts.dropna()\n",
    "results = {}\n",
    "results[''] = ['MSE', 'MAE', 'MAPE']\n",
    "results['entsoe'] = [mean_squared_error(forecasts['actual'], forecasts['entsoe']), \n",
    "                     mean_absolute_error(forecasts['actual'], forecasts['entsoe']),\n",
    "                     mean_absolute_percentage_error(forecasts['actual'], forecasts['entsoe'])\n",
    "                    ]\n",
    "results['m6-all'] = [mean_squared_error(forecasts['actual'], forecasts['model6']), \n",
    "                     mean_absolute_error(forecasts['actual'], forecasts['model6']),\n",
    "                     mean_absolute_percentage_error(forecasts['actual'], forecasts['model6'])\n",
    "                    ]\n",
    "results['m3-calendar'] = [mean_squared_error(forecasts['actual'], forecasts['model3']), \n",
    "                          mean_absolute_error(forecasts['actual'], forecasts['model3']),\n",
    "                          mean_absolute_percentage_error(forecasts['actual'], forecasts['model3'])\n",
    "                         ]\n",
    "\n",
    "print(tabulate(results, headers='keys', numalign=\"right\", tablefmt='latex', floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(forecasts.index, forecasts['entsoe'], label='ENTSOE Forecast')\n",
    "plt.plot(forecasts.index, forecasts['actual'], label='Actual Load')\n",
    "plt.plot(forecasts.index, forecasts['model6'], label='Model 6 (All)')\n",
    "#plt.plot(forecasts.index, forecasts['model3'], label='Model 3 (Calendar)')\n",
    "plt.title('Forecast Comparison: Rolling Forecast')\n",
    "plt.ylabel('Electricity load (in MW)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
