{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weather Only\n",
    "As previously seen in the data analysis, if we assume a loss function with mean squared error, we could actually improve the forecast by just adding a certain value.\n",
    "As showed in the theoretical part, we will concetrating on minimizing the mean squared error and evaluation will be based both on the root mean squared (forecast) error (RMSE) and the MAPE (Mean Average Percentag Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activating auto-logging. Current session state plus future input saved.\n",
      "Filename       : logs/notebook_04_20170507_2215.py\n",
      "Mode           : rotate\n",
      "Output logging : True\n",
      "Raw input log  : False\n",
      "Timestamping   : True\n",
      "State          : active\n"
     ]
    }
   ],
   "source": [
    "# module imports\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "import time as t\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from numpy import newaxis\n",
    "\n",
    "\n",
    "# Setup for Latex Export: https://matplotlib.org/users/pgf.html. Need to import before pyplot\n",
    "def figsize(scale):\n",
    "    fig_width_pt = 469.755                          # Get this from LaTeX using \\the\\textwidth\n",
    "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
    "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
    "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
    "    fig_height = fig_width*golden_mean              # height in inches\n",
    "    fig_size = [fig_width,fig_height]\n",
    "    return fig_size\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.use('pgf')\n",
    "pgf_with_rc_fonts = {\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"axes.labelsize\": 10,               # LaTeX default is 10pt font.\n",
    "    \"font.size\": 10,\n",
    "    \"legend.fontsize\": 8,               # Make the legend/label fonts a little smaller\n",
    "    \"xtick.labelsize\": 8,\n",
    "    \"ytick.labelsize\": 8,\n",
    "    \"figure.figsize\": figsize(0.9),     # default fig size of 0.9 textwidth\n",
    "    #\"font.serif\": [],                   # use latex default serif font\n",
    "    #\"font.sans-serif\": [\"DejaVu Sans\"], # use a specific sans-serif font\n",
    "}\n",
    "mpl.rcParams.update(pgf_with_rc_fonts)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from statsmodels.tsa import stattools\n",
    "from tabulate import tabulate\n",
    "\n",
    "import math\n",
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, LSTM\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "#from seasonal import fit_seasons, adjust_seasons\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "%matplotlib notebook\n",
    "mpl.rcParams['figure.figsize'] = (9,5)\n",
    "\n",
    "# Make plots nicer\n",
    "#import seaborn\n",
    "#%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Import custom module functions\n",
    "module_path = os.path.abspath(os.path.join(''))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from helper_functions import data\n",
    "from helper_functions import lstm\n",
    "\n",
    "# Set up logging. Create path for log file\n",
    "if not os.path.exists('logs/'):\n",
    "    os.makedirs('logs')\n",
    "logname = 'logs/notebook_04_' + t.strftime(\"%Y%m%d_%H%M\") + '.py'\n",
    "mode = 'rotate'\n",
    "options = '-o -t'\n",
    "%logstart $options $logname $mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model run configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Which features from the dataset should be loaded:\n",
    "# ['all', 'actual', 'entsoe', 'weather_t', 'weather_i', 'holiday', 'weekday', 'hour', 'month']\n",
    "features = ['weather_t', 'weather_i']\n",
    "\n",
    "# How the data should get splitted into training (+validation) and test\n",
    "splits = [0.8]\n",
    "validation_split = 0.2\n",
    "\n",
    "# How many epochs in total\n",
    "epochs = 50\n",
    "# Set verbosity level. 0 for only per model, 1 for progress bar...\n",
    "verbose = 0\n",
    "\n",
    "# Output files\n",
    "model_name = 'model4_'\n",
    "res_dir = 'results/notebook_04/'\n",
    "plot_dir = 'plots/notebook_04/'\n",
    "model_dir = 'models/notebook_04/'\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "results = pd.DataFrame(columns=['model_name', 'config', 'dropout',\n",
    "                                'train_loss', 'train_rmse', 'train_mae', 'train_mape', \n",
    "                                'valid_loss', 'valid_rmse', 'valid_mae', 'valid_mape', \n",
    "                                'test_rmse', 'test_mae', 'test_mape',\n",
    "                                'epochs', 'batch_train', 'input_shape',\n",
    "                                'total_time', 'time_step', 'splits'\n",
    "                               ])\n",
    "output_table = res_dir + model_name + 'results_' + t.strftime(\"%Y%m%d\") + '.csv'\n",
    "test_output_table = res_dir + model_name + 'test_results' + t.strftime(\"%Y%m%d\") + '.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======================================\n",
      "| Loaded dataset  | standardized                                 |\n",
      "| File path       | /home/ubuntu/STLF/Data/fulldataset_stand.csv |\n",
      "| Loaded features | ['weather_t', 'weather_i']                   |\n",
      "| Dataset Shape   | (20231, 32)                                  |\n",
      "=======================================\n",
      "| Loaded dataset  | standardized                                 |\n",
      "| File path       | /home/ubuntu/STLF/Data/fulldataset_stand.csv |\n",
      "| Loaded features | ['actual']                                   |\n",
      "| Dataset Shape   | (20231, 1)                                   |\n",
      "=======================================\n",
      "| Loaded dataset  | standardized                                 |\n",
      "| File path       | /home/ubuntu/STLF/Data/fulldataset_stand.csv |\n",
      "| Loaded features | ['entsoe_fc']                                |\n",
      "| Dataset Shape   | (20231, 1)                                   |\n",
      "=======================================\n",
      "| Original dataset shape  | (20231, 32) |\n",
      "| 1. new dataset of shape | (16184, 32) |\n",
      "| 2. new dataset of shape | (4047, 32)  |\n",
      "=======================================\n",
      "| Original dataset shape  | (20231, 1) |\n",
      "| 1. new dataset of shape | (16184, 1) |\n",
      "| 2. new dataset of shape | (4047, 1)  |\n",
      "=======================================\n",
      "| Original dataset shape  | (20231, 1) |\n",
      "| 1. new dataset of shape | (16184, 1) |\n",
      "| 2. new dataset of shape | (4047, 1)  |\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "df_X = data.load_data(typ='standardized', features=features)\n",
    "# actual values\n",
    "df_y = data.load_data(typ='standardized', features=['actual'])\n",
    "# The benchmark, which will be used to compare the model's forecast on the testdata\n",
    "df_bench = data.load_data(typ='standardized', features=['entsoe'])\n",
    "\n",
    "split_X = data.split_series(series=df_X, mode='percentage', splits=splits)\n",
    "split_y = data.split_series(series=df_y, mode='percentage', splits=splits)\n",
    "split_bench = data.split_series(series=df_bench, mode='percentage', splits=splits)\n",
    "\n",
    "time_train = split_y[0].index\n",
    "time_test = split_y[1].index\n",
    "\n",
    "X_train = split_X[0].values\n",
    "X_test = split_X[1].values\n",
    "\n",
    "y_train = split_y[0].values\n",
    "y_test = split_y[1].values\n",
    "\n",
    "benchmark_test = split_bench[1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: 1- 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\n",
      "| Number of model configs generated | 648 |\n"
     ]
    }
   ],
   "source": [
    "# Layers: Param: Stateful --> True, \n",
    "layer_conf = [ True, True, True ]\n",
    "\n",
    "cells = [[ 10, 20, 24, 50, 100, 150 ], [0, 10, 20, 50], [0, 10, 20]]\n",
    "dropout = [0, 0.1, 0.2]\n",
    "batch_size = [8, 16, 64]\n",
    "timesteps = [1]\n",
    "# Based on these inputs, it will generate all possible combinations\n",
    "#cells = [ [ 20, 30, 50 ], [ 0, 5, 10, 20 ] ]\n",
    "#dropout = [ 0, 0.1, 0.2 ]\n",
    "#batch_size = [ 1, 16, 32, 64 ]\n",
    "#timesteps = [ 1 ]\n",
    "\n",
    "models = []\n",
    "models = lstm.generate_combinations(\n",
    "    model_name=model_name, layer_conf=layer_conf, cells=cells, dropout=dropout, \n",
    "    batch_size=batch_size, timesteps=timesteps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running through all generated models\n",
    "Note: Depending on the above settings, this can take very long!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Model 1/648 =========================\n",
      "| Starting with model | model4_1_l-10              |\n",
      "| Starting time       | 2017-05-07 22:15:26.746315 |\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 48.10889744758606 |\n",
      "| Training loss & MAE              | 0.794 | 0.6296156054254441      |\n",
      "| Validation loss & mae            | 0.330 | 0.4654259867322298      |\n",
      "========================= Model 2/648 =========================\n",
      "| Starting with model | model4_2_l-10              |\n",
      "| Starting time       | 2017-05-07 22:16:15.007730 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 8 of 16184 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 27.554410457611084 |\n",
      "| Training loss & MAE              | 0.806 | 0.629747707557914        |\n",
      "| Validation loss & mae            | 0.320 | 0.4502654018821103       |\n",
      "========================= Model 3/648 =========================\n",
      "| Starting with model | model4_3_l-10              |\n",
      "| Starting time       | 2017-05-07 22:16:42.666109 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 56 of 16184 number of obs.\n",
      "Effective validation split now is: 0.198\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 11.242179870605469 |\n",
      "| Training loss & MAE              | 0.916 | 0.6868353475733558       |\n",
      "| Validation loss & mae            | 0.418 | 0.5089851248264313       |\n",
      "========================= Model 4/648 =========================\n",
      "| Starting with model | model4_4_l-10_d-0.1        |\n",
      "| Starting time       | 2017-05-07 22:16:54.089872 |\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 3.000 | Time: 46.53169560432434 |\n",
      "| Training loss & MAE              | 0.832 | 0.6518721203785965      |\n",
      "| Validation loss & mae            | 0.347 | 0.48014907380680977     |\n",
      "========================= Model 5/648 =========================\n",
      "| Starting with model | model4_5_l-10_d-0.1        |\n",
      "| Starting time       | 2017-05-07 22:17:40.928204 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 8 of 16184 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 27.01839303970337 |\n",
      "| Training loss & MAE              | 0.829 | 0.6412909832611514      |\n",
      "| Validation loss & mae            | 0.327 | 0.45491050220657103     |\n",
      "========================= Model 6/648 =========================\n",
      "| Starting with model | model4_6_l-10_d-0.1        |\n",
      "| Starting time       | 2017-05-07 22:18:08.208706 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 56 of 16184 number of obs.\n",
      "Effective validation split now is: 0.198\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 10.12805438041687 |\n",
      "| Training loss & MAE              | 0.931 | 0.6940978501397784      |\n",
      "| Validation loss & mae            | 0.427 | 0.5145120906829834      |\n",
      "========================= Model 7/648 =========================\n",
      "| Starting with model | model4_7_l-10_d-0.2        |\n",
      "| Starting time       | 2017-05-07 22:18:18.703867 |\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 4.000 | Time: 47.84583330154419 |\n",
      "| Training loss & MAE              | 0.852 | 0.6590385713070005      |\n",
      "| Validation loss & mae            | 0.363 | 0.48638673656516607     |\n",
      "========================= Model 8/648 =========================\n",
      "| Starting with model | model4_8_l-10_d-0.2        |\n",
      "| Starting time       | 2017-05-07 22:19:06.975419 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 8 of 16184 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 28.251811027526855 |\n",
      "| Training loss & MAE              | 0.851 | 0.6539636343459706       |\n",
      "| Validation loss & mae            | 0.333 | 0.4669415451364942       |\n",
      "========================= Model 9/648 =========================\n",
      "| Starting with model | model4_9_l-10_d-0.2        |\n",
      "| Starting time       | 2017-05-07 22:19:35.673751 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 56 of 16184 number of obs.\n",
      "Effective validation split now is: 0.198\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 4.000 | Time: 11.1004478931427 |\n",
      "| Training loss & MAE              | 0.953 | 0.7064335501134986     |\n",
      "| Validation loss & mae            | 0.413 | 0.5086212909221649     |\n",
      "========================= Model 10/648 =========================\n",
      "| Starting with model | model4_10_l-10_l-10        |\n",
      "| Starting time       | 2017-05-07 22:19:47.228270 |\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 2.000 | Time: 76.8718318939209 |\n",
      "| Training loss & MAE              | 0.815 | 0.6483386480519997     |\n",
      "| Validation loss & mae            | 0.334 | 0.46385154965115183    |\n",
      "========================= Model 11/648 =========================\n",
      "| Starting with model | model4_11_l-10_l-10        |\n",
      "| Starting time       | 2017-05-07 22:21:04.858561 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 8 of 16184 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 39.29307746887207 |\n",
      "| Training loss & MAE              | 0.818 | 0.6339484655997072      |\n",
      "| Validation loss & mae            | 0.313 | 0.4489233218649826      |\n",
      "========================= Model 12/648 =========================\n",
      "| Starting with model | model4_12_l-10_l-10        |\n",
      "| Starting time       | 2017-05-07 22:21:44.835697 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 56 of 16184 number of obs.\n",
      "Effective validation split now is: 0.198\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 16.749929904937744 |\n",
      "| Training loss & MAE              | 0.967 | 0.7163459590756067       |\n",
      "| Validation loss & mae            | 0.358 | 0.4874921190738678       |\n",
      "========================= Model 13/648 =========================\n",
      "| Starting with model | model4_13_l-10_l-10_d-0.1  |\n",
      "| Starting time       | 2017-05-07 22:22:02.639350 |\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 3.000 | Time: 72.93720054626465 |\n",
      "| Training loss & MAE              | 0.819 | 0.6428057544983288      |\n",
      "| Validation loss & mae            | 0.355 | 0.47963160773118335     |\n",
      "========================= Model 14/648 =========================\n",
      "| Starting with model | model4_14_l-10_l-10_d-0.1  |\n",
      "| Starting time       | 2017-05-07 22:23:16.430427 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 8 of 16184 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 42.40521216392517 |\n",
      "| Training loss & MAE              | 0.827 | 0.6430686962766612      |\n",
      "| Validation loss & mae            | 0.352 | 0.4829056850901925      |\n",
      "========================= Model 15/648 =========================\n",
      "| Starting with model | model4_15_l-10_l-10_d-0.1  |\n",
      "| Starting time       | 2017-05-07 22:23:59.856843 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 56 of 16184 number of obs.\n",
      "Effective validation split now is: 0.198\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 17.719731092453003 |\n",
      "| Training loss & MAE              | 0.997 | 0.731534000227947        |\n",
      "| Validation loss & mae            | 0.442 | 0.5251103621721268       |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================= Model 16/648 =========================\n",
      "| Starting with model | model4_16_l-10_l-10_d-0.2  |\n",
      "| Starting time       | 2017-05-07 22:24:18.639647 |\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 78.52063536643982 |\n",
      "| Training loss & MAE              | 0.812 | 0.6366178285312115      |\n",
      "| Validation loss & mae            | 0.318 | 0.45580216258028405     |\n",
      "========================= Model 17/648 =========================\n",
      "| Starting with model | model4_17_l-10_l-10_d-0.2  |\n",
      "| Starting time       | 2017-05-07 22:25:38.331574 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 8 of 16184 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 44.81069016456604 |\n",
      "| Training loss & MAE              | 0.816 | 0.6374352885265138      |\n",
      "| Validation loss & mae            | 0.311 | 0.4500672939685312      |\n",
      "========================= Model 18/648 =========================\n",
      "| Starting with model | model4_18_l-10_l-10_d-0.2  |\n",
      "| Starting time       | 2017-05-07 22:26:24.406380 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 56 of 16184 number of obs.\n",
      "Effective validation split now is: 0.198\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 17.816454887390137 |\n",
      "| Training loss & MAE              | 1.028 | 0.7467928590750931       |\n",
      "| Validation loss & mae            | 0.466 | 0.5451397150754929       |\n",
      "========================= Model 19/648 =========================\n",
      "| Starting with model | model4_19_l-10_l-20        |\n",
      "| Starting time       | 2017-05-07 22:26:43.741660 |\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 84.37216711044312 |\n",
      "| Training loss & MAE              | 0.761 | 0.6128568264353003      |\n",
      "| Validation loss & mae            | 0.332 | 0.464811917035668       |\n",
      "========================= Model 20/648 =========================\n",
      "| Starting with model | model4_20_l-10_l-20        |\n",
      "| Starting time       | 2017-05-07 22:28:09.755773 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 8 of 16184 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 43.67843270301819 |\n",
      "| Training loss & MAE              | 0.834 | 0.6516649917191717      |\n",
      "| Validation loss & mae            | 0.334 | 0.466416935076808       |\n",
      "========================= Model 21/648 =========================\n",
      "| Starting with model | model4_21_l-10_l-20        |\n",
      "| Starting time       | 2017-05-07 22:28:55.241518 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 56 of 16184 number of obs.\n",
      "Effective validation split now is: 0.198\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 5.000 | Time: 19.810797214508057 |\n",
      "| Training loss & MAE              | 0.981 | 0.7221479741948666       |\n",
      "| Validation loss & mae            | 0.370 | 0.4842838364839554       |\n",
      "========================= Model 22/648 =========================\n",
      "| Starting with model | model4_22_l-10_l-20_d-0.1  |\n",
      "| Starting time       | 2017-05-07 22:29:16.726877 |\n",
      "______________________________________________________________________\n",
      "| Minimum validation loss at epoch | 2.000 | Time: 84.93445444107056 |\n",
      "| Training loss & MAE              | 0.800 | 0.6384815445494445      |\n",
      "| Validation loss & mae            | 0.325 | 0.459996605452932       |\n",
      "========================= Model 23/648 =========================\n",
      "| Starting with model | model4_23_l-10_l-20_d-0.1  |\n",
      "| Starting time       | 2017-05-07 22:30:43.871923 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 8 of 16184 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "=============== ERROR 23/648 =============\n",
      "| Model:  | model4_23_l-10_l-20_d-0.1                                                                                                                                                                                                                                |\n",
      "| Config: | {'name': 'model4_23_l-10_l-20_d-0.1', 'layers': [{'type': 'lstm', 'cells': 10, 'dropout': 0.1, 'stateful': True, 'ret_seq': True}, {'type': 'lstm', 'cells': 20, 'dropout': 0.1, 'stateful': True, 'ret_seq': False}], 'batch_size': 16, 'timesteps': 1} |\n",
      "Error: [Errno 2] No such file or directory: '/home/ubuntu/STLF/cmr8'\n",
      "========================= Model 24/648 =========================\n",
      "| Starting with model | model4_24_l-10_l-20_d-0.1  |\n",
      "| Starting time       | 2017-05-07 22:31:30.125762 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 56 of 16184 number of obs.\n",
      "Effective validation split now is: 0.198\n",
      "=============== ERROR 24/648 =============\n",
      "| Model:  | model4_24_l-10_l-20_d-0.1                                                                                                                                                                                                                                |\n",
      "| Config: | {'name': 'model4_24_l-10_l-20_d-0.1', 'layers': [{'type': 'lstm', 'cells': 10, 'dropout': 0.1, 'stateful': True, 'ret_seq': True}, {'type': 'lstm', 'cells': 20, 'dropout': 0.1, 'stateful': True, 'ret_seq': False}], 'batch_size': 64, 'timesteps': 1} |\n",
      "Error: [Errno 2] No such file or directory: '/home/ubuntu/STLF/cmr8'\n",
      "========================= Model 25/648 =========================\n",
      "| Starting with model | model4_25_l-10_l-20_d-0.2  |\n",
      "| Starting time       | 2017-05-07 22:31:47.396030 |\n",
      "=============== ERROR 25/648 =============\n",
      "| Model:  | model4_25_l-10_l-20_d-0.2                                                                                                                                                                                                                               |\n",
      "| Config: | {'name': 'model4_25_l-10_l-20_d-0.2', 'layers': [{'type': 'lstm', 'cells': 10, 'dropout': 0.2, 'stateful': True, 'ret_seq': True}, {'type': 'lstm', 'cells': 20, 'dropout': 0.2, 'stateful': True, 'ret_seq': False}], 'batch_size': 8, 'timesteps': 1} |\n",
      "Error: [Errno 2] No such file or directory: '/home/ubuntu/STLF/cmr8'\n",
      "========================= Model 26/648 =========================\n",
      "| Starting with model | model4_26_l-10_l-20_d-0.2  |\n",
      "| Starting time       | 2017-05-07 22:33:12.954258 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 8 of 16184 number of obs.\n",
      "Effective validation split now is: 0.200\n",
      "=============== ERROR 26/648 =============\n",
      "| Model:  | model4_26_l-10_l-20_d-0.2                                                                                                                                                                                                                                |\n",
      "| Config: | {'name': 'model4_26_l-10_l-20_d-0.2', 'layers': [{'type': 'lstm', 'cells': 10, 'dropout': 0.2, 'stateful': True, 'ret_seq': True}, {'type': 'lstm', 'cells': 20, 'dropout': 0.2, 'stateful': True, 'ret_seq': False}], 'batch_size': 16, 'timesteps': 1} |\n",
      "Error: [Errno 2] No such file or directory: '/home/ubuntu/STLF/cmr8'\n",
      "========================= Model 27/648 =========================\n",
      "| Starting with model | model4_27_l-10_l-20_d-0.2  |\n",
      "| Starting time       | 2017-05-07 22:33:59.200926 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 56 of 16184 number of obs.\n",
      "Effective validation split now is: 0.198\n",
      "=============== ERROR 27/648 =============\n",
      "| Model:  | model4_27_l-10_l-20_d-0.2                                                                                                                                                                                                                                |\n",
      "| Config: | {'name': 'model4_27_l-10_l-20_d-0.2', 'layers': [{'type': 'lstm', 'cells': 10, 'dropout': 0.2, 'stateful': True, 'ret_seq': True}, {'type': 'lstm', 'cells': 20, 'dropout': 0.2, 'stateful': True, 'ret_seq': False}], 'batch_size': 64, 'timesteps': 1} |\n",
      "Error: [Errno 2] No such file or directory: '/home/ubuntu/STLF/cmr8'\n",
      "========================= Model 28/648 =========================\n",
      "| Starting with model | model4_28_l-10_l-10        |\n",
      "| Starting time       | 2017-05-07 22:34:22.691085 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== ERROR 28/648 =============\n",
      "| Model:  | model4_28_l-10_l-10                                                                                                                                                                                                                           |\n",
      "| Config: | {'name': 'model4_28_l-10_l-10', 'layers': [{'type': 'lstm', 'cells': 10, 'dropout': 0, 'stateful': True, 'ret_seq': True}, {'type': 'lstm', 'cells': 10, 'dropout': 0, 'stateful': True, 'ret_seq': False}], 'batch_size': 8, 'timesteps': 1} |\n",
      "Error: [Errno 2] No such file or directory: '/home/ubuntu/STLF/cmr8'\n",
      "========================= Model 29/648 =========================\n",
      "| Starting with model | model4_29_l-10_l-10        |\n",
      "| Starting time       | 2017-05-07 22:35:49.531720 |\n",
      "Warnining: Division \"sample_size/batch_size\" not a natural number.\n",
      "Dropped the last 8 of 16184 number of obs.\n",
      "Effective validation split now is: 0.200\n"
     ]
    }
   ],
   "source": [
    "start_time = t.time()\n",
    "for idx, m in enumerate(models):\n",
    "    stopper = t.time()\n",
    "    print('========================= Model {}/{} ========================='.format(idx+1, len(models)))\n",
    "    print(tabulate([['Starting with model', m['name']], ['Starting time', datetime.fromtimestamp(stopper)]],\n",
    "                   tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "    try:\n",
    "        # Creating the Keras Model\n",
    "        model = lstm.create_model(layers=m['layers'], sample_size=X_train.shape[0], batch_size=m['batch_size'], \n",
    "                          timesteps=m['timesteps'], features=X_train.shape[1])\n",
    "        # Training...\n",
    "        history = lstm.train_model(model=model, mode='fit', y=y_train, X=X_train, \n",
    "                                   batch_size=m['batch_size'], timesteps=m['timesteps'], epochs=epochs, \n",
    "                                   rearrange=False, validation_split=validation_split, verbose=verbose)\n",
    "\n",
    "        # Generating plots...\n",
    "        lstm.plot_history(model_config=m, history=history, path=plot_dir, display=False)\n",
    "        \n",
    "        # Write results\n",
    "        min_loss = np.min(history.history['val_loss'])\n",
    "        min_idx = np.argmin(history.history['val_loss'])\n",
    "        min_epoch = min_idx + 1\n",
    "        \n",
    "        print('______________________________________________________________________')\n",
    "        print(tabulate([['Minimum validation loss at epoch', min_epoch, 'Time: {}'.format(t.time()-stopper)],\n",
    "                        ['Training loss & MAE', history.history['loss'][min_idx], history.history['mean_absolute_error'][min_idx]  ], \n",
    "                        ['Validation loss & mae', history.history['val_loss'][min_idx], history.history['val_mean_absolute_error'][min_idx] ],\n",
    "                       ], tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "        \n",
    "        \n",
    "        result = [{'model_name': m['name'], 'config': m, 'train_loss': history.history['loss'][min_idx], 'train_rmse': 0,\n",
    "                   'train_mae': history.history['mean_absolute_error'][min_idx], 'train_mape': 0,\n",
    "                   'valid_loss': history.history['val_loss'][min_idx], 'valid_rmse': 0, \n",
    "                   'valid_mae': history.history['val_mean_absolute_error'][min_idx],'valid_mape': 0, \n",
    "                   'test_rmse': 0, 'test_mae': 0, 'test_mape': 0, 'epochs': '{}/{}'.format(min_epoch, epochs), 'batch_train':m['batch_size'],\n",
    "                   'input_shape':(X_train.shape[0], timesteps, X_train.shape[1]), 'total_time':t.time()-stopper, \n",
    "                   'time_step':0, 'splits':splits, 'dropout': m['layers'][0]['dropout']\n",
    "                  }]\n",
    "        results = results.append(result, ignore_index=True)\n",
    "        \n",
    "        # Saving the weights\n",
    "        model.save(model_dir + m['name'] + '.h5')\n",
    "        \n",
    "        if not os.path.isfile(output_table):\n",
    "            results.to_csv(output_table, sep=';')\n",
    "        else: # else it exists so append without writing the header\n",
    "            results.to_csv(output_table,mode = 'a',header=False, sep=';')\n",
    "        \n",
    "    # Shouldn't catch all errors, but for now...\n",
    "    except BaseException as e:\n",
    "        print('=============== ERROR {}/{} ============='.format(idx+1, len(models)))\n",
    "        print(tabulate([['Model:', m['name']], ['Config:', m]], tablefmt=\"jira\", numalign=\"right\", floatfmt=\".3f\"))\n",
    "        print('Error: {}'.format(e))\n",
    "        result = [{'model_name': m['name'], 'config': m, 'train_loss': str(e)}]\n",
    "        results = results.append(result, ignore_index=True)\n",
    "        results.to_csv(output_table,sep=';')\n",
    "        continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model selection based on MAE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Mean Absolute Error:\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#mean-absolute-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selection = 5\n",
    "top_models = results.nsmallest(selection, 'valid_mae')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the top models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Config\n",
    "test_results = pd.DataFrame(columns=['Model name', 'Mean absolute error', 'Mean squared error', 'Diff. MAE', 'Diff. MSE'])\n",
    "\n",
    "# Create ENTSOE benchmark for the forecast error\n",
    "# TODO: For a fair comparison: We would need to cut the benchmark aswell. But as there are multiple models, we cant really...\n",
    "mse_entsoe = mean_squared_error(y_test, benchmark_test)\n",
    "mae_entsoe = mean_absolute_error(y_test, benchmark_test)\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "for index, row in top_models.iterrows():\n",
    "    filename = model_dir + row['model_name'] + '.h5'\n",
    "    model = load_model(filename)\n",
    "    batch_size = int(row['batch_train'])\n",
    "    \n",
    "    # Calculate scores\n",
    "    loss, mae = lstm.evaluate_model(model=model, X=X_test, y=y_test, batch_size=batch_size, timesteps=1, verbose=verbose)\n",
    "    \n",
    "    # Store results\n",
    "    result = [{'Model name': row['model_name'], \n",
    "               'Mean squared error': loss, 'Mean absolute error': mae,\n",
    "               'Diff. MAE': mae - mae_entsoe, 'Diff. MSE': loss - mse_entsoe\n",
    "              }]\n",
    "    test_results = test_results.append(result, ignore_index=True)\n",
    "    \n",
    "    # Generate predictions\n",
    "    model.reset_states()\n",
    "    model_predictions = lstm.get_predictions(model=model, X=X_test, batch_size=batch_size, timesteps=timesteps[0], verbose=verbose)\n",
    "    \n",
    "    # Plot\n",
    "    predictions[row['model_name']] = model_predictions\n",
    "    \n",
    "\n",
    "test_results = test_results.sort_values('Mean absolute error', ascending=True)\n",
    "test_results = test_results.set_index(['Model name'])\n",
    "\n",
    "if not os.path.isfile(test_output_table):\n",
    "    test_results.to_csv(test_output_table, sep=';')\n",
    "else: # else it exists so append without writing the header\n",
    "    test_results.to_csv(test_output_table,mode = 'a',header=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test dataset performance of the best {} (out of {} tested models):'.format(min(selection, len(models)), len(models)))\n",
    "print('ENTSOE Forecast (Benchmark) metrics: \\tMAE = {:.3f}  \\tMSE = {:.3f}'.format(np.asscalar(mae_entsoe), np.asscalar(mse_entsoe)))\n",
    "print(tabulate(test_results, headers='keys', tablefmt=\"grid\", numalign=\"right\", floatfmt=\".3f\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform to original values (Destandardization)\n",
    "\n",
    "$$z = \\frac{x - \\mu}{\\sigma} \\rightarrow x = z\\sigma + \\mu $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = test_results.index[0]\n",
    "\n",
    "# Load standardization params\n",
    "columns = ['actual', 'entsoe_fc','bsl_t','brn_t','zrh_t','lug_t','lau_t','gen_t','stg_t','luz_t']\n",
    "params_mu = read_csv(os.path.join('Data', 'standardization_params_mu.csv'), header=None)\n",
    "params_mu.columns = columns\n",
    "params_sigma = read_csv(os.path.join('Data', 'standardization_params_sigma.csv'), header=None)\n",
    "params_sigma.columns = columns\n",
    "\n",
    "mu = params_mu.loc[0]['actual']\n",
    "sigma = params_sigma.loc[0]['actual']\n",
    "\n",
    "y_test_raw = np.round(y_test * sigma + mu)\n",
    "predictions_raw = np.round(predictions[best_model] * sigma + mu)\n",
    "benchmark_test_raw = np.round(benchmark_test * sigma + mu)\n",
    "\n",
    "size = predictions_raw.shape[0]\n",
    "mse_entsoe = mean_squared_error(y_test_raw[0:size], benchmark_test_raw[0:size])\n",
    "mae_entsoe = mean_absolute_error(y_test_raw[0:size], benchmark_test_raw[0:size])\n",
    "\n",
    "mse = mean_squared_error(y_test_raw[0:size], predictions_raw)\n",
    "mae = mean_absolute_error(y_test_raw[0:size], predictions_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time_vector = time_test.values\n",
    "time_vector = time_vector[0:size]\n",
    "time_vector = np.reshape(time_vector, (size,1))\n",
    "\n",
    "plt.clf()\n",
    "plt.ion()\n",
    "\n",
    "#%matplotlib qt\n",
    "plt.plot(time_vector, benchmark_test_raw[:size], label='ENTSOE Forecast')\n",
    "plt.plot(time_vector, y_test_raw[:size], label='Actual Load')\n",
    "plt.plot(time_vector, predictions_raw, label='Model predictions')\n",
    "plt.title('LSTM Model using the ENTSOE Forecast as input')\n",
    "plt.ylabel('Electricity load (in MW)')\n",
    "plt.xlabel('Date')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show\n",
    "\n",
    "filename = plot_dir + model_name + 'top_model_predictions'\n",
    "plt.savefig(filename + '.pgf')\n",
    "plt.savefig(filename + '.pdf')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bad Comparison?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
